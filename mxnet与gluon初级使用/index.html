<!DOCTYPE html><html lang="en"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>mxnet与gluon初级使用 | Welcome To Oa</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="/css/donate.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/6.0.0/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.2/pure-min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.2/grids-responsive-min.css"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/3.2.1/jquery.min.js"></script><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">mxnet与gluon初级使用</h1><a id="logo" href="/.">Welcome To Oa</a><p class="description"></p></div><div id="nav-menu"><a href="/." class="current"><i class="fa fa-home"> Home</i></a><a href="/archives/"><i class="fa fa-archive"> Archive</i></a><a href="/about/"><i class="fa fa-user"> About</i></a><a href="/guestbook/"><i class="fa fa-comments"> Guestbook</i></a><a href="/atom.xml"><i class="fa fa-rss"> RSS</i></a></div></div><div id="layout" class="pure-g"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">mxnet与gluon初级使用</h1><div class="post-meta">May 29, 2018<script src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> Hits</span></span></div><div class="clear"><div id="toc" class="toc-article"><div class="toc-title">Contents</div><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#undefined"><span class="toc-number">1.</span> <span class="toc-text">Installation</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#undefined"><span class="toc-number">2.</span> <span class="toc-text">DataType</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#undefined"><span class="toc-number">2.1.</span> <span class="toc-text">NDArray Basic</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#undefined"><span class="toc-number">2.1.1.</span> <span class="toc-text">NDArray创建</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#undefined"><span class="toc-number">2.1.2.</span> <span class="toc-text">NDArray运算</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#undefined"><span class="toc-number">2.1.3.</span> <span class="toc-text">索引</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#undefined"><span class="toc-number">2.2.</span> <span class="toc-text">NDArray 梯度</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#undefined"><span class="toc-number">3.</span> <span class="toc-text">LoadData</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#undefined"><span class="toc-number">4.</span> <span class="toc-text">Define Net</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#undefined"><span class="toc-number">5.</span> <span class="toc-text">define loss</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#undefined"><span class="toc-number">6.</span> <span class="toc-text">train</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#undefined"><span class="toc-number">7.</span> <span class="toc-text">Conclusion</span></a></li></ol></div></div><div class="post-content"><p>因为要入门DL，而DL之所以成为DL就是因为他网络的复杂性与深度，单纯使用python实现是极为困难的，于是出现了<code>caffe</code>，<code>tensorflow</code>，<code>mxnet</code>，<code>pytorch</code>等框架，这里我们选择了<code>mxnet</code>为实践框架，教程参考于<a href="https://zh.gluon.ai/" target="_blank" rel="noopener">https://zh.gluon.ai/</a></p>
<h1>Installation</h1>
<p>这个直接参照<a href="https://zh.gluon.ai/chapter_crashcourse/install.html" target="_blank" rel="noopener">教程</a>即可，里面有提到各种方法的安装，这里写这个标题是因为，一般情况下我们是将环境与代码安装与运行在服务器上的，但是我们查看jupter-notebook一般是在本地浏览器查看，这个时候我们可以使用端口转发</p>
<p><code>ssh -L 8888:localhost:8000 user@ip</code></p>
<p>这句命令是将远程ip的8888端口映射到本地的8000端口，这时候我们就可以直接在本地访问<code>localhost:8000</code>再输入对应的token即可查看和运行其中的代码</p>
<h1>DataType</h1>
<p>在DL中，大部分时间我们都是对数据进行操作，那么就涉及到了数据的类型，由于不同的数据类型定义所支持的操作以及操作效率是不同的，所以MXNET实现了NDArray可以支持CPU和GPU的异步计算，以及自动求导，从而使DL实现过程更加便捷。</p>
<h2>NDArray Basic</h2>
<p>在大多数使用的时候与<code>numpy.array</code>类似，但还是做些简要的提及</p>
<h3>NDArray创建</h3>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> nd</span><br><span class="line">x=nd.arange(<span class="number">12</span>)</span><br><span class="line">x.reshape((<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line">nd.zeros((<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>))</span><br><span class="line">nd.ones((<span class="number">2</span>,<span class="number">3</span>))</span><br><span class="line">nd.array([[<span class="number">1</span>,<span class="number">2</span>],[<span class="number">3</span>,<span class="number">4</span>]])</span><br><span class="line">nd.random.normal(<span class="number">0</span>,<span class="number">1</span>,shape=(<span class="number">3</span>,<span class="number">4</span>))</span><br></pre></td></tr></table></figure></p>
<p>创建基本等同与<code>np.array</code>的创建方式，所以就不做解释了。
加一点，<code>np.array</code>与<code>nd.array</code>也是可以相互变换的。</p>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">x = np.ones((<span class="number">2</span>, <span class="number">3</span>))</span><br><span class="line">y = nd.array(x)  <span class="comment"># NumPy To NDArray。</span></span><br><span class="line">z = y.asnumpy()  <span class="comment"># NDArray To NumPy。</span></span><br></pre></td></tr></table></figure></p>
<h3>NDArray运算</h3>
<p>这里也是与<code>np.array</code>相似的，我在下面列举下，x，y均是<code>nd.array</code>类型。</p>
<ol>
<li><code>x+y</code>：对应位置元素相加</li>
<li><code>x*y</code>：对应位置元素相乘</li>
<li><code>x/y</code>：对应位置元素想除</li>
<li><code>x.exp()</code>：所有位置元素做指数运算</li>
<li><code>x==y</code>：对应位置元素做比较，相等为1，否则为0</li>
<li><code>nd.dot(x,y.T)</code>：x，y.T作矩阵乘法</li>
<li><code>x.sum()</code>：对x中所有元素求和</li>
<li><code>x.norm()</code>：求x的二范式</li>
</ol>
<h3>索引</h3>
<p>这里不做介绍，与<code>np.array</code>的索引/切片用法一致，如<code>x[1:3,2:4]</code>这样。</p>
<h2>NDArray 梯度</h2>
<p>在mxnet中想求梯度还是很简单的。假设我们需要求<code>f(x)</code>关于<code>x</code>的导数（<code>x</code> 是<code>nd.array</code>）</p>
<ol start="0">
<li><code>from mxnet import autograd</code> 导入求梯度的库</li>
<li><code>x.attach_grad()</code> 申请内存存储梯度</li>
<li><code>with autograd.record():c = f(a)</code>记录计算图</li>
<li><code>c.backward()</code>求出梯度</li>
<li><code>x.grad</code> 访问梯度</li>
</ol>
<h1>LoadData</h1>
<p>这里我们使用Gulon来将载入数据。</p>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> gluonbook <span class="keyword">as</span> gb</span><br><span class="line"><span class="keyword">from</span> mxnet <span class="keyword">import</span> autograd, gluon, init, nd</span><br><span class="line"><span class="keyword">from</span> mxnet.gluon <span class="keyword">import</span> loss <span class="keyword">as</span> gloss, nn</span><br><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">train_iter, test_iter = gb.load_data_fashion_mnist(batch_size)</span><br></pre></td></tr></table></figure></p>
<h1>Define Net</h1>
<p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># 无需设置输入层，自动判断输入层的节点数</span><br><span class="line">net = nn.Sequential()</span><br><span class="line"># 添加隐藏层和输出层，并定义激活函数</span><br><span class="line">net.add(nn.Dense(256, activation=&apos;relu&apos;))</span><br><span class="line">net.add(nn.Dense(10))</span><br><span class="line">net.add(nn.Dense(10))</span><br><span class="line"># 初始化权重</span><br><span class="line">net.initialize(init.Normal(sigma=0.01))</span><br></pre></td></tr></table></figure></p>
<p>为了处理过拟合问题，我们通常在定义网络时使用dropout方法，即以一定概率使节点失活。
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">drop_prob = <span class="number">0.5</span></span><br><span class="line"><span class="comment"># add dropout layer with prob equals drop_prob</span></span><br><span class="line">net.add(nn.Dropout(drop_prob))</span><br></pre></td></tr></table></figure></p>
<h1>define loss</h1>
<p>gluon中有定义很多个损失函数，如下：</p>
<p><img src="/mxnet与gluon初级使用/1.png" alt="lossfuncs"></p>
<p>具体的定义参见<a href="https://mxnet.incubator.apache.org/api/python/gluon/loss.html?highlight=loss#module-mxnet.gluon.loss" target="_blank" rel="noopener">loss_api</a></p>
<p>但我们也可以自己来定义loss function。可以在这个过程中加入正则化，防止过拟合。</p>
<h1>train</h1>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">loss = gloss.SoftmaxCrossEntropyLoss()</span><br><span class="line">trainer = gluon.Trainer(net.collect_params(), <span class="string">'sgd'</span>, &#123;<span class="string">'learning_rate'</span>: <span class="number">0.5</span>&#125;)</span><br><span class="line">num_epochs = <span class="number">5</span></span><br><span class="line">gb.train_cpu(net, train_iter, test_iter, loss, num_epochs, batch_size,</span><br><span class="line">             <span class="keyword">None</span>, <span class="keyword">None</span>, trainer)</span><br></pre></td></tr></table></figure></p>
<p>在trainer中我们先收集参数，再定义优化方法为<code>sgd</code>，并设置lr，而在训练时设置网络结构，训练集，测试集，loss，迭代次数，批大小，以及训练器。</p>
<h1>Conclusion</h1>
<p>一般处理的过程也是读取数据，定义网络结构，初始化参数，设置损失函数，训练这样的流程了。后面将会以多种CNN网络为例子详细介绍。</p>
</div><script type="text/javascript" src="/js/share.js?v=0.0.0" async></script><a data-url="https://jarvis.world/mxnet与gluon初级使用/" data-id="cjpjx3q2z000wq8ihc1g7ap0b" class="article-share-link">Share</a><div class="tags"><a href="/tags/DL/">DL</a><a href="/tags/mxnet/">mxnet</a><a href="/tags/gluon/">gluon</a></div><div class="post-nav"><a href="/python-and-opencv-basics/" class="pre">python opencv 基础</a><a href="/mongodb-import-complex-txt/" class="next">mongodb import complex txt</a></div><div id="container"></div><link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css"><script src="https://imsun.github.io/gitment/dist/gitment.browser.js"></script><script>var gitment = new Gitment({
  id: 'mxnet与gluon初级使用',
  owner: 'Jarvis-K',
  repo: 'Jarvis-K.github.io',
  oauth: {
    client_id: 'f5c6bbd5b2ae20c2f136',
    client_secret: 'fed7dc675f5293f90d56d4055654fb89db9003a8',
  },
})
gitment.render('container')</script></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank" class="search-form"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="https://jarvis.world"/></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> Categories</i></div></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> Tags</i></div><div class="tagcloud"><a href="/tags/python/" style="font-size: 15px;">python</a> <a href="/tags/installation/" style="font-size: 15px;">installation</a> <a href="/tags/Math/" style="font-size: 15px;">Math</a> <a href="/tags/DL/" style="font-size: 15px;">DL</a> <a href="/tags/rl-papers/" style="font-size: 15px;">rl-papers</a> <a href="/tags/mongodb/" style="font-size: 15px;">mongodb</a> <a href="/tags/util/" style="font-size: 15px;">util</a> <a href="/tags/Util/" style="font-size: 15px;">Util</a> <a href="/tags/mxnet/" style="font-size: 15px;">mxnet</a> <a href="/tags/gluon/" style="font-size: 15px;">gluon</a> <a href="/tags/pyqt/" style="font-size: 15px;">pyqt</a> <a href="/tags/cv/" style="font-size: 15px;">cv</a> <a href="/tags/数值计算与优化/" style="font-size: 15px;">数值计算与优化</a> <a href="/tags/ML/" style="font-size: 15px;">ML</a> <a href="/tags/DM/" style="font-size: 15px;">DM</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> Recent</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/Summary-Mean-Field-Multi-Agent-Reinforcement-Learning/">Summary: Mean Field Multi-Agent Reinforcement Learning. ICML(2018)</a></li><li class="post-list-item"><a class="post-list-link" href="/Summary-ACCNet-Actor-Coordinator-Critic-Net-for-“Learning-to-Communicate”-with-Deep-Multi-agent-Reinforcement-Learning/">Summary: ACCNet: Actor-Coordinator-Critic Net for “Learning-to-Communicate” with Deep Multi-agent Reinforcement Learning</a></li><li class="post-list-item"><a class="post-list-link" href="/Summary-Emergence-of-Grounded-Compositional-Language-in-Multi-Agent-Populations/">Summary: Emergence of Grounded Compositional Language in Multi-Agent Populations(AAAI 2018)</a></li><li class="post-list-item"><a class="post-list-link" href="/Summary-Learning-Multiagent-Communication-with-Backpropagation/">Summary: Learning Multiagent Communication with Backpropagation(Nips 2016)</a></li><li class="post-list-item"><a class="post-list-link" href="/Summary-Learning-to-Communicate-with-Deep-Multi-Agent-Reinforcement-Learning/">Summary: Learning to Communicate with Deep Multi-Agent Reinforcement Learning(Nips 2016)</a></li><li class="post-list-item"><a class="post-list-link" href="/Summary-TarMAC-Targeted-Multi-Agent-Communication/">Summary: TarMAC:Targeted Multi-Agent Communication</a></li><li class="post-list-item"><a class="post-list-link" href="/Summary-Intrinsic-Social-Motivation-Via-Causal-Influence-In-Multi-Agent-RL/">Summary: Intrinsic Social Motivation Via Causal Influence In Multi-Agent RL</a></li><li class="post-list-item"><a class="post-list-link" href="/Summary-Actor-Attention-Critic-For-Multi-Agent-Reinforcement-Learning/">Summary: Actor-Attention-Critic For Multi-Agent Reinforcement Learning</a></li><li class="post-list-item"><a class="post-list-link" href="/Summary-Learning-Attentional-Communication-for-Multi-Agent-Cooperation/">Summary: Learning Attentional Communication for Multi-Agent Cooperation</a></li><li class="post-list-item"><a class="post-list-link" href="/Summary-ACM-Learning-Dynamic-Multi-agent-Cooperation-via-Attentional-Commmunication-Model/">Summary: ACM: Learning  Dynamic Multi-agent Cooperation via Attentional Commmunication Model(ICANN 2018)</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> Links</i></div><ul></ul><a href="https://cww97.cn/" title="cww" target="_blank">cww</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2018 <a href="/." rel="nofollow">Welcome To Oa.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a id="rocket" href="#top" class="show"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.pack.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="/css/jquery.fancybox.css?v=0.0.0"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>