<!DOCTYPE html><html lang="en"><head><meta http-equiv="content-type" content="text/html; charset=utf-8"><meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport"><meta content="yes" name="apple-mobile-web-app-capable"><meta content="black-translucent" name="apple-mobile-web-app-status-bar-style"><meta content="telephone=no" name="format-detection"><meta name="description"><title>Summary: Mean Field Multi-Agent Reinforcement Learning. ICML(2018) | Welcome To Oa</title><link rel="stylesheet" type="text/css" href="/css/style.css?v=0.0.0"><link rel="stylesheet" type="text/css" href="/css/donate.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/normalize/6.0.0/normalize.min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.2/pure-min.css"><link rel="stylesheet" type="text/css" href="//cdn.bootcss.com/pure/0.6.2/grids-responsive-min.css"><link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css"><script type="text/javascript" src="//cdn.bootcss.com/jquery/3.2.1/jquery.min.js"></script><link rel="Shortcut Icon" type="image/x-icon" href="/favicon.ico"><link rel="apple-touch-icon" href="/apple-touch-icon.png"><link rel="apple-touch-icon-precomposed" href="/apple-touch-icon.png"></head><body><div class="body_container"><div id="header"><div class="site-name"><h1 class="hidden">Summary: Mean Field Multi-Agent Reinforcement Learning. ICML(2018)</h1><a id="logo" href="/.">Welcome To Oa</a><p class="description"></p></div><div id="nav-menu"><a href="/." class="current"><i class="fa fa-home"> Home</i></a><a href="/archives/"><i class="fa fa-archive"> Archive</i></a><a href="/about/"><i class="fa fa-user"> About</i></a><a href="/guestbook/"><i class="fa fa-comments"> Guestbook</i></a><a href="/atom.xml"><i class="fa fa-rss"> RSS</i></a></div></div><div id="layout" class="pure-g"><div class="pure-u-1 pure-u-md-3-4"><div class="content_container"><div class="post"><h1 class="post-title">Summary: Mean Field Multi-Agent Reinforcement Learning. ICML(2018)</h1><div class="post-meta">Dec 3, 2018<script src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js" async></script><span id="busuanzi_container_page_pv"> | <span id="busuanzi_value_page_pv"></span><span> Hits</span></span></div><div class="clear"><div id="toc" class="toc-article"><div class="toc-title">Contents</div><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#undefined"><span class="toc-number">1.</span> <span class="toc-text">Abstract</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#undefined"><span class="toc-number">2.</span> <span class="toc-text">Introduction</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#undefined"><span class="toc-number">2.1.</span> <span class="toc-text">Background</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#undefined"><span class="toc-number">2.2.</span> <span class="toc-text">Related works</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#undefined"><span class="toc-number">3.</span> <span class="toc-text">Approach</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#undefined"><span class="toc-number">3.1.</span> <span class="toc-text">Loss function</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#undefined"><span class="toc-number">4.</span> <span class="toc-text">Algorithms</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#undefined"><span class="toc-number">5.</span> <span class="toc-text">Experiments</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#undefined"><span class="toc-number">5.1.</span> <span class="toc-text">Settings:</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#undefined"><span class="toc-number">5.2.</span> <span class="toc-text">Results</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#undefined"><span class="toc-number">6.</span> <span class="toc-text">Conclusion</span></a></li></ol></div></div><div class="post-content"><h1>Abstract</h1>
<p>Author of the paper: University College London and Shanghai Jiao Tong University</p>
<ul>
<li>Background: Existing MARL methods becomes intractable due to the curse of dimensionality and the exponential growth of agent interactions.</li>
<li>Thoughts: approximate interactions within the population of agents with those between a single agent and the average effect from the overall population or neighboring agents.</li>
<li>Propose mean field Q-Learning and mean field Actor-critic algorithms which can be apply to many agent environments.</li>
<li><a href="https://arxiv.org/pdf/1802.05438.pdf" target="_blank" rel="noopener">paper</a></li>
</ul>
<h1>Introduction</h1>
<h2>Background</h2>
<p>Multi-agent reinforcement learning(MARL) is concerned with a set of autonomous agents that share a common environment. Independant Q-Learning that considering other agents as part of environments often fails as the changes of one agent will affect that of the others which make learning unstable.</p>
<h2>Related works</h2>
<p>For MARL part:</p>
<ul>
<li>Studies show that an agent who learns the effect of joint actions has better performance than those who do not in many scenarios.</li>
<li>Solutions to address the nonstationary issue in MARL: opponent modeling, policy parameter sharing, centralized training with decentralized execution(MADDPG).</li>
</ul>
<p>The above approaches limit their studies mostly to tens of agents, due to :</p>
<ul>
<li>the input space of Q grows exponentially with the number of agents grows.</li>
<li>the accumulated noises by the exploratory actions of other agents make the Q learning not feasible.</li>
</ul>
<p>Also related to Mean Field Game(MFG):</p>
<ul>
<li>studies population behaviors resulting from the aggregations of decisions taken from individuals.</li>
<li>Yang combines MFG with RL to learn reward fuction and forward mean dynamics in Inverse RL.</li>
</ul>
<p>But this paper's goal is to form a computable Q-learning algorithm.</p>
<h1>Approach</h1>
<p>To address the issue that $Q^j(s,a)$ become infeasible when the number of agents grow large,the author factorize it as follow:
$Q ^ { j } ( s , \boldsymbol { a } ) = \frac { 1 } { N ^ { j } } \sum _ { k \in \mathcal { N } ( j ) } Q ^ { j } \left( s , a ^ { j } , a ^ { k } \right)$</p>
<p>and $a ^ { k } = \overline { a } ^ { j } + \delta a ^ { j , k }$ where $\overline { a } ^ { j } = \frac { 1 } { N ^ { j } } \sum _ { k } a ^ { k }$</p>
<p>combine above we can get:</p>
<p><img src="/Summary-Mean-Field-Multi-Agent-Reinforcement-Learning/appr.png" alt=""></p>
<p>For calculating Q-Value.</p>
<p>$Q _ { t + 1 } ^ { j } \left( s , a ^ { j } , \overline { a } ^ { j } \right) = ( 1 - \alpha ) Q _ { t } ^ { j } \left( s , a ^ { j } , \overline { a } ^ { j } \right) + \alpha \left[ r ^ { j } + \gamma v _ { t } ^ { j } \left( s ^ { \prime } ,\overline{a} \right) \right]$</p>
<p>$v _ { t } ^ { j } \left( s ^ { \prime } \right) = \sum _ { a ^ { j } } \pi _ { t } ^ { j } \left( a ^ { j } | s ^ { \prime } , \overline { a } ^ { j } \right) \mathbb { E } _ { \overline { a } ^ { j } ( \boldsymbol { a } - j ) \sim \pi _ { t } ^ { - j } } \left[ Q _ { t } ^ { j } \left( s ^ { \prime } , a ^ { j } , \overline { a } ^ { j } \right) \right]$</p>
<h2>Loss function</h2>
<p>For MFQ:
$\mathscr { L } \left( \phi ^ { j } \right) = \left( y ^ { j } - Q _ { \phi ^ { j } } \left( s , a ^ { j } , \overline { a } ^ { j } \right) \right) ^ { 2 }$</p>
<p>$y ^ { j } = r ^ { j } + \gamma v _ { \phi _ { - } ^ { j } } ^ { \mathrm { MF } } \left( s ^ { \prime } \right)$</p>
<h1>Algorithms</h1>
<p><img src="/Summary-Mean-Field-Multi-Agent-Reinforcement-Learning/algo.png" alt=""></p>
<h1>Experiments</h1>
<p>This paper evaluate algorithm in 3 different scenarios, but here I only show the battle game example which is more related to my works.</p>
<h2>Settings:</h2>
<p><img src="/Summary-Mean-Field-Multi-Agent-Reinforcement-Learning/battle.png" alt=""></p>
<p>Two armies fighting against each other in a grid world.The goal of each army is to get more rewards.Action space contains move or attack nearby agents.
Reward setting is :
-0.005 for every move, 0.2 for attacking an enemy,5 for killing an enemy, -0.1 for attacking an empty grid, -0.1 for being attacked or killed.</p>
<h2>Results</h2>
<p>2000 rounds after self-plays trainning.
<img src="/Summary-Mean-Field-Multi-Agent-Reinforcement-Learning/res.png" alt=""></p>
<ul>
<li>IL performs better than AC and MF-AC imply the effictiveness of off-policy learning with replay-buffer.</li>
<li>The replay-buffer and the maximum operator in calculating Q-values may be the reason why MFQ converge fast than MFAC .</li>
</ul>
<h1>Conclusion</h1>
<p>Transform many-body problem into two-body problem using mean field theory enable the scalability in MARL.</p>
</div><script type="text/javascript" src="/js/share.js?v=0.0.0" async></script><a data-url="https://jarvis.world/Summary-Mean-Field-Multi-Agent-Reinforcement-Learning/" data-id="cjv2m69b9000r1xih29n1njfs" class="article-share-link">Share</a><div class="tags"><a href="/tags/rl-papers/">rl-papers</a></div><div class="post-nav"><a href="/EM-Algorithm/" class="pre">EM Algorithm</a><a href="/Summary-ACCNet-Actor-Coordinator-Critic-Net-for-“Learning-to-Communicate”-with-Deep-Multi-agent-Reinforcement-Learning/" class="next">Summary: ACCNet: Actor-Coordinator-Critic Net for “Learning-to-Communicate” with Deep Multi-agent Reinforcement Learning</a></div><div id="container"></div><link rel="stylesheet" href="https://imsun.github.io/gitment/style/default.css"><script src="https://imsun.github.io/gitment/dist/gitment.browser.js"></script><script>var gitment = new Gitment({
  id: 'Summary: Mean Field Multi-Agent Reinforcement Learning. ICML(2018)',
  owner: 'Jarvis-K',
  repo: 'Jarvis-K.github.io',
  oauth: {
    client_id: 'f5c6bbd5b2ae20c2f136',
    client_secret: 'fed7dc675f5293f90d56d4055654fb89db9003a8',
  },
})
gitment.render('container')</script></div></div></div><div class="pure-u-1-4 hidden_mid_and_down"><div id="sidebar"><div class="widget"><form action="//www.google.com/search" method="get" accept-charset="utf-8" target="_blank" class="search-form"><input type="text" name="q" maxlength="20" placeholder="Search"/><input type="hidden" name="sitesearch" value="https://jarvis.world"/></form></div><div class="widget"><div class="widget-title"><i class="fa fa-folder-o"> Categories</i></div></div><div class="widget"><div class="widget-title"><i class="fa fa-star-o"> Tags</i></div><div class="tagcloud"><a href="/tags/util/" style="font-size: 15px;">util</a> <a href="/tags/Math/" style="font-size: 15px;">Math</a> <a href="/tags/ml-algo/" style="font-size: 15px;">ml-algo</a> <a href="/tags/gnn-papers/" style="font-size: 15px;">gnn-papers</a> <a href="/tags/installation/" style="font-size: 15px;">installation</a> <a href="/tags/Util/" style="font-size: 15px;">Util</a> <a href="/tags/rl-papers/" style="font-size: 15px;">rl-papers</a> <a href="/tags/mongodb/" style="font-size: 15px;">mongodb</a> <a href="/tags/DL/" style="font-size: 15px;">DL</a> <a href="/tags/python/" style="font-size: 15px;">python</a> <a href="/tags/mxnet/" style="font-size: 15px;">mxnet</a> <a href="/tags/gluon/" style="font-size: 15px;">gluon</a> <a href="/tags/pyqt/" style="font-size: 15px;">pyqt</a> <a href="/tags/cv/" style="font-size: 15px;">cv</a> <a href="/tags/数值计算与优化/" style="font-size: 15px;">数值计算与优化</a> <a href="/tags/ML/" style="font-size: 15px;">ML</a> <a href="/tags/DM/" style="font-size: 15px;">DM</a></div></div><div class="widget"><div class="widget-title"><i class="fa fa-file-o"> Recent</i></div><ul class="post-list"><li class="post-list-item"><a class="post-list-link" href="/Hypergraph-Convolution-and-Hyper-Attention/">Hypergraph Convolution and Hyper Attention</a></li><li class="post-list-item"><a class="post-list-link" href="/EM-Algorithm/">EM Algorithm</a></li><li class="post-list-item"><a class="post-list-link" href="/Summary-Mean-Field-Multi-Agent-Reinforcement-Learning/">Summary: Mean Field Multi-Agent Reinforcement Learning. ICML(2018)</a></li><li class="post-list-item"><a class="post-list-link" href="/Summary-ACCNet-Actor-Coordinator-Critic-Net-for-“Learning-to-Communicate”-with-Deep-Multi-agent-Reinforcement-Learning/">Summary: ACCNet: Actor-Coordinator-Critic Net for “Learning-to-Communicate” with Deep Multi-agent Reinforcement Learning</a></li><li class="post-list-item"><a class="post-list-link" href="/Summary-Emergence-of-Grounded-Compositional-Language-in-Multi-Agent-Populations/">Summary: Emergence of Grounded Compositional Language in Multi-Agent Populations(AAAI 2018)</a></li><li class="post-list-item"><a class="post-list-link" href="/Summary-Learning-Multiagent-Communication-with-Backpropagation/">Summary: Learning Multiagent Communication with Backpropagation(Nips 2016)</a></li><li class="post-list-item"><a class="post-list-link" href="/Summary-Learning-to-Communicate-with-Deep-Multi-Agent-Reinforcement-Learning/">Summary: Learning to Communicate with Deep Multi-Agent Reinforcement Learning(Nips 2016)</a></li><li class="post-list-item"><a class="post-list-link" href="/Summary-TarMAC-Targeted-Multi-Agent-Communication/">Summary: TarMAC:Targeted Multi-Agent Communication</a></li><li class="post-list-item"><a class="post-list-link" href="/Summary-Intrinsic-Social-Motivation-Via-Causal-Influence-In-Multi-Agent-RL/">Summary: Intrinsic Social Motivation Via Causal Influence In Multi-Agent RL</a></li><li class="post-list-item"><a class="post-list-link" href="/Summary-Actor-Attention-Critic-For-Multi-Agent-Reinforcement-Learning/">Summary: Actor-Attention-Critic For Multi-Agent Reinforcement Learning</a></li></ul></div><div class="widget"><div class="widget-title"><i class="fa fa-external-link"> Links</i></div><ul></ul><a href="https://cww97.cn/" title="cww" target="_blank">cww</a></div></div></div><div class="pure-u-1 pure-u-md-3-4"><div id="footer">Copyright © 2019 <a href="/." rel="nofollow">Welcome To Oa.</a> Powered by<a rel="nofollow" target="_blank" href="https://hexo.io"> Hexo.</a><a rel="nofollow" target="_blank" href="https://github.com/tufu9441/maupassant-hexo"> Theme</a> by<a rel="nofollow" target="_blank" href="https://github.com/pagecho"> Cho.</a></div></div></div><a id="rocket" href="#top" class="show"></a><script type="text/javascript" src="/js/totop.js?v=0.0.0" async></script><script type="text/javascript" src="//cdn.bootcss.com/fancybox/2.1.5/jquery.fancybox.pack.js" async></script><script type="text/javascript" src="/js/fancybox.js?v=0.0.0" async></script><link rel="stylesheet" type="text/css" href="/css/jquery.fancybox.css?v=0.0.0"><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
</script><script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML" async></script><script type="text/javascript" src="/js/codeblock-resizer.js?v=0.0.0"></script><script type="text/javascript" src="/js/smartresize.js?v=0.0.0"></script></div></body></html>